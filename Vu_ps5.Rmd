---
title: "POLSCI 630 - Problem Set 5"
author: "Huong Vu"
date: "2/5/2022"
output: pdf_document
---

Before starting the problem set, I loaded the packages and set the work directory for the file by going to the tab Session and then Set Working Directory. 

```{r setup, include = F, message=F, warning=F, tidy=TRUE, tidy.opts=list(width.cutoff=60)} 
# set universal settings for all chunks
knitr::opts_chunk$set(warning = F, message = F,      # don't print warnings or messages. 
                      fig.width = 5, fig.height = 4) # format all figures as 5x4 inches.
library(tidyverse)
library(jtools)
library(npreg)
library(Metrics)
library(dplyr)
library(margins)
library(dotwhisker)
```

# 1.Calculate MSE

## 1.1 The true function is $-x^2+3x$ 
## 1.2 I created a simulated dataset and 2 sets of data, including a training dataset and a test dataset.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}

set.seed(630) # the set.seed command to ensure that others who run your code 
# will get the same results

# Generate 3000 observations of x such that it has a normal distribution with 
# mean 0 and a standard deviation of 3
x <- rnorm(3000, mean = 0, sd = 3)

# Generate corresponding y such that it has a normal distribution with mean 
# equal to the true function given above and a standard deviation of 50
y <- sapply(x, function(x) rnorm(1, mean = 3*x - x^2, sd = 50))

# Combine x and y generated above into a data frame
data_set <- data.frame(x,y)

# Extract out 1500 observations to create a training dataset & 1500 observations 
# to create a test dataset
data_set$group <- sample(
  factor(rep(1:2, length.out=nrow(data_set)),
  labels=paste0("Group", 1:2))
  
)

train <- data_set %>%
  filter(group == "Group1")

test <- data_set %>%
  filter(group == "Group2")
```

I created the plots of the test and training datasets to see how they are distributed.

```{r}
# plot for the training dataset
ggplot() + geom_point(data = train, aes(x = x, y = y), size = 1) + ggtitle("Training Set")
```

```{r}
# plot for the test dataset
ggplot() + geom_point(data = test, aes(x = x, y = y), color = "purple", shape = 17, size = 1) + ggtitle("Test Set")
```

## 1.3 I fitted the following models to the training data: linear regression (x), quadratic regression (x2), cubic regression (x3), and exponential regression; and used `summ()` function to report the results.

```{r}
# linear regression
fit_1 <- lm(y~x, data = train)
summ(fit_1)

# quadratic regression
fit_2 <- lm(y ~ poly(x, degree = 2), data = train)
summ(fit_2)

# cubic regression
fit_3 <- lm(y ~ poly(x, degree = 3), data = train)
summ(fit_3)

# exponential regression
fit_4 <- lm(y ~ exp(x), data = train)
summ(fit_4)

```

## 1.4 & 1.5 Plot the regression lines for all four regressions with the training dataset and the test dataset

```{r}
# Create a dataframe of x's (ranges from -9 to 9 with 0.01 increment) that will 
# serve as the input to the predict function.
x_predict <- seq(from = -9, to = 9, by = 0.01)

# Predict y values for the input of x's specified in the last line using the 
# parameter(s) estimated from each estimation method.
y_predict_1 <- predict(fit_1, newdata = data.frame(x = x_predict))
y_predict_2 <- predict(fit_2, newdata = data.frame(x = x_predict))
y_predict_3 <- predict(fit_3, newdata = data.frame(x = x_predict))
y_predict_4 <- predict(fit_4, newdata = data.frame(x = x_predict))

# Create a dataframe with x and the predicted values from all the models
lines <- data.frame(x_predict, y_predict_1, y_predict_2, y_predict_3, y_predict_4)
```

The plot with the regression lines of all four regressions for the training dataset

```{r}
ggplot() + geom_point(data = train, aes(x = x,
y = y, color = "Train data"), size = 1) +
geom_line(data = lines, aes(x = x_predict,
y = y_predict_1, color = "Linear Regression"),
size = 0.8) + geom_line(data = lines,
aes(x = x_predict, y = y_predict_2, color = "Quadratic Regression"),
size = 0.8) + geom_line(data = lines,
aes(x = x_predict, y = y_predict_3, color = "Cubic Regression"),
size = 0.8) + geom_line(data = lines,
aes(x = x_predict, y = y_predict_4, color = "Exponential Regression"),
size = 0.2) + scale_color_manual(values = c("green",
"blue", "yellow", "red", "black")) + ggtitle("Training Set") +
ylim(-250, 250)
```

The plot with the regression lines of all four regressions for the test dataset

```{r}
ggplot() + geom_point(data = test, aes(x = x, y = y, color = "Test data"),
shape = 17, size = 1) + geom_line(data = lines, aes(x = x_predict,
y = y_predict_1, color = "Linear Regression"),
size = 0.8) + geom_line(data = lines, aes(x = x_predict,
y = y_predict_2, color = "Quadratic Regression"), size = 0.8) +
geom_line(data = lines, aes(x = x_predict, y = y_predict_3,
color = "Cubic Regression"), size = 0.8) + geom_line(data = lines,
aes(x = x_predict, y = y_predict_4, color = "Exponential Regression"),
size = 0.2) + scale_color_manual(values = c("green", "blue",
"yellow", "red", "purple")) + ggtitle("Test Set") + ylim(-250,
250)
```

## 1.6 Calculate and report training MSEs for all four models (linear, quadratic, cubic, and exponential) manually

```{r}

# Predict y values for the input of x's specified in the last line using the 
# parameter(s) estimated from each estimation method.
y_predict_1 <- predict(fit_1, newdata = data.frame(x = x_predict))

y_predict_1_training <- predict(fit_1, data.frame(x=train$x))
y_predict_2_training <- predict(fit_2, data.frame(x=train$x))
y_predict_3_training <- predict(fit_3, data.frame(x=train$x))
y_predict_4_training <- predict(fit_4, data.frame(x=train$x))

# calculate training MSE for the linear regression model
(sum((train$y - y_predict_1_training)^2))/length(y_predict_1_training)

# calculate training MSE for the quadratic regression model
(sum((train$y - y_predict_2_training)^2))/length(y_predict_2_training)

# calculate training MSE for the cubic regression model
(sum((train$y - y_predict_3_training)^2))/length(y_predict_3_training)

# calculate training MSE for the exponential regression model
(sum((train$y - y_predict_4_training)^2))/length(y_predict_4_training)

```

I verified the results of training MSE by using `mse()` function from Metrics package.

```{r}
mse(train$y , y_predict_1_training)
mse(train$y , y_predict_2_training)
mse(train$y , y_predict_3_training)
mse(train$y , y_predict_4_training)
```
## 1.7 Calculate and report test MSEs for all four models (linear, quadratic, cubic, and exponential) manually

```{r}
y_predict_1_test <- predict(fit_1, data.frame(x=test$x))
y_predict_2_test <- predict(fit_2, data.frame(x=test$x))
y_predict_3_test <- predict(fit_3, data.frame(x=test$x))
y_predict_4_test <- predict(fit_4, data.frame(x=test$x))

# calculate training MSE for the linear regression model
mean((test$y - y_predict_1_test)^2)

# calculate training MSE for the quadratic regression model
mean((test$y - y_predict_2_test)^2)

# calculate training MSE for the cubic regression model
mean((test$y - y_predict_3_test)^2)

# calculate training MSE for the exponential regression model
mean((test$y - y_predict_4_test)^2)
```

I verified the results of test MSE by using `mse()` function from Metrics package.

```{r}
mse(test$y , y_predict_1_test)
mse(test$y , y_predict_2_test)
mse(test$y , y_predict_3_test)
mse(test$y , y_predict_4_test)
```

## 1.8 Which model (linear, quadratic, cubic, or exponential) performs best prediction-wise for the training data? Which model (linear, quadratic, cubic, or exponential) performs best prediction-wise for the test data? What is the basis of these decisions?

MSE (mean squared error) is the average squared difference between a prediction and its true value. MSE is composed of bias, variance, and irreducible error. The small MSE indicates that the model performs best prediction-wise.

* For the training data, the cubic regression model performs the best prediction (MSE = 2495.93)
* For the test data, the quadratic regression model performs the best prediction (MSE = 2602.206)


# 2. Log Transformation and Multiple Regression

I loaded `VOTE1` dataset and named it `vote`.

```{r, results="hide", results="hide", message=F, warning=F}
vote <- read_csv("VOTE1.csv") # load the data
summary(vote) # quick glimpses at the data
```

## 2.1 Calculate new variable for the natural log of `expendA` and call it `lexpendA`

```{r}
# Add logged variables to `vote`
vote <- vote %>%
  mutate(lexpendA = log(expendA))      
print(vote)

```

## 2.2 Regress `voteA` on `lexpendA` with lm() and report the result with summ()

Interpret the coefficient on `lexpendA`: A 1% change in campaign expenditure results in *beta_1*/100 = 0.065 %  of vote change for candidate A

```{r}
m1 <- lm(voteA ~ lexpendA, data = vote)
summ(m1)
```

## 2.3 Regress voteA on lexpendA with lm() while controlling for the percentage of vote that went to candidate Aâ€™s party. Report the result with summ(). 

Interpret the coefficient on `lexpendA`: A 1% change in campaign expenditure results in *beta_1*/100 = 0.0596 %  of vote change for candidate A, keeping `prtystrA` variable constant.

```{r}
m2 <- lm(voteA ~ lexpendA + prtystrA, data = vote)
summ(m2)
```

## 2.4 Plot the marginal effect of expenditures from model in part 3 on vote share of candidate A. 

Is the marginal effect of expenditures on vote share constant across all values of expenditures? No, the marginal effect of expenditures is not constant. There are diminishing returns between expenditures and vote share. After some optimal level of expenditures, the additional expenditures will not increase the vote share.

```{r}
m3 <- lm(voteA ~ log(expendA), data = vote)

cplot(m3, "expendA", what = "effect", main = "Marginal effect by expenditures")

```

## 2.5 Plot the marginal effect of ln(expenditures) from model in part 3 on vote share of candidate A. 

Is the marginal effect of expenditures on vote share constant across all values of ln(expenditures)? Yes, the marginal effect of expenditures constants across all values of ln(expenditures).

```{r}
cplot(m1, "lexpendA", what = "effect", main = "Marginal effect by expenditures")
```


